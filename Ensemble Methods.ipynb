{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataX = np.load('train_data.npy')\n",
    "test_dataX = np.load('test_data.npy')\n",
    "train_datay = pd.read_csv('train_labels.csv')\n",
    "\n",
    "train_datay = train_datay.drop('Id',1)\n",
    "\n",
    "#splitting data for validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dataX,train_datay, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\python installed\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7393           27.18m\n",
      "         2           0.7392           27.67m\n",
      "         3           0.7390           26.87m\n",
      "         4           0.7388           26.69m\n",
      "         5           0.7386           26.16m\n",
      "         6           0.7385           25.50m\n",
      "         7           0.7383           24.90m\n",
      "         8           0.7381           24.30m\n",
      "         9           0.7379           23.70m\n",
      "        10           0.7378           23.11m\n",
      "        20           0.7360           17.32m\n",
      "        30           0.7343           11.35m\n",
      "        40           0.7326            5.66m\n",
      "        50           0.7309            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.0001, loss='deviance', max_depth=8,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(learning_rate=0.0001, n_estimators=50, max_depth=8,verbose=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.8780094  0.1219906 ]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87432414 0.12567586]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8766304  0.1233696 ]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87597033 0.12402967]\n",
      " [0.87790075 0.12209925]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87839688 0.12160312]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87829853 0.12170147]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87613096 0.12386904]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8744894  0.1255106 ]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87480389 0.12519611]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87674295 0.12325705]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87730723 0.12269277]\n",
      " [0.87601192 0.12398808]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87675327 0.12324673]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87449153 0.12550847]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8766304  0.1233696 ]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87908614 0.12091386]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87827477 0.12172523]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87888891 0.12111109]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.8786907  0.1213093 ]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87560779 0.12439221]\n",
      " [0.87429603 0.12570397]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8780094  0.1219906 ]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8768086  0.1231914 ]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.8766304  0.1233696 ]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87606777 0.12393223]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87597033 0.12402967]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87869228 0.12130772]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87839763 0.12160237]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87460145 0.12539855]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87904278 0.12095722]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bad input shape (400, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4d5c30d6d837>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mroc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ROC:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python installed\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    354\u001b[0m     return _average_binary_score(\n\u001b[0;32m    355\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python installed\\lib\\site-packages\\sklearn\\metrics\\base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python installed\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[1;32m--> 328\u001b[1;33m                                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python installed\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \"\"\"\n\u001b[0;32m    617\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[1;32m--> 618\u001b[1;33m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python installed\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python installed\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (400, 2)"
     ]
    }
   ],
   "source": [
    "Y_test_pred = model.predict_proba(X_test)\n",
    "print(Y_test_pred)\n",
    "\n",
    "roc = roc_auc_score(y_test, Y_test_pred)\n",
    "print(\"ROC:\" + str(round(roc,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.8780094  0.1219906 ]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87432414 0.12567586]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8766304  0.1233696 ]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87597033 0.12402967]\n",
      " [0.87790075 0.12209925]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87839688 0.12160312]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87829853 0.12170147]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87613096 0.12386904]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8744894  0.1255106 ]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87480389 0.12519611]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87674295 0.12325705]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87730723 0.12269277]\n",
      " [0.87601192 0.12398808]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87675327 0.12324673]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87449153 0.12550847]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8766304  0.1233696 ]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87908614 0.12091386]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87827477 0.12172523]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87888891 0.12111109]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.8786907  0.1213093 ]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87428932 0.12571068]\n",
      " [0.87560779 0.12439221]\n",
      " [0.87429603 0.12570397]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8780094  0.1219906 ]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.8768086  0.1231914 ]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.8766304  0.1233696 ]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87606777 0.12393223]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87597033 0.12402967]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87485932 0.12514068]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87869228 0.12130772]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87419391 0.12580609]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87839763 0.12160237]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87876676 0.12123324]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87460145 0.12539855]\n",
      " [0.87918298 0.12081702]\n",
      " [0.87917532 0.12082468]\n",
      " [0.87904278 0.12095722]\n",
      " [0.87904278 0.12095722]]\n"
     ]
    }
   ],
   "source": [
    "Y_test_predClass = model.predict_proba(X_test)\n",
    "print(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_test_predClass[:,0]).to_csv('submission.csv', header=[\"Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataX = np.load('train_data.npy')\n",
    "test_dataX = np.load('test_data.npy')\n",
    "train_datay = pd.read_csv('train_labels.csv')\n",
    "\n",
    "train_datay = train_datay.drop('Id',1)\n",
    "\n",
    "#splitting data for validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dataX,train_datay, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\python installed\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "d:\\programs\\python installed\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    7.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(verbose=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.  1. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.6 0.4]\n",
      " [1.  0. ]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.4 0.6]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.1 0.9]\n",
      " [0.6 0.4]\n",
      " [0.9 0.1]\n",
      " [0.7 0.3]\n",
      " [0.  1. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.4 0.6]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.4 0.6]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.6 0.4]\n",
      " [0.4 0.6]\n",
      " [0.7 0.3]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.  1. ]\n",
      " [0.3 0.7]\n",
      " [0.5 0.5]\n",
      " [0.8 0.2]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [1.  0. ]\n",
      " [0.3 0.7]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.1 0.9]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [0.8 0.2]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [0.1 0.9]\n",
      " [1.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.7 0.3]\n",
      " [0.4 0.6]\n",
      " [0.7 0.3]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  0. ]\n",
      " [0.1 0.9]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [0.  1. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.2 0.8]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.3 0.7]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.1 0.9]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.2 0.8]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.2 0.8]\n",
      " [0.  1. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.4 0.6]\n",
      " [0.9 0.1]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [0.2 0.8]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.1 0.9]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.3 0.7]\n",
      " [0.6 0.4]\n",
      " [0.4 0.6]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.7 0.3]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.6 0.4]\n",
      " [0.4 0.6]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [0.  1. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [1.  0. ]\n",
      " [0.8 0.2]\n",
      " [0.9 0.1]\n",
      " [0.9 0.1]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]\n",
      " [0.4 0.6]\n",
      " [0.9 0.1]\n",
      " [0.8 0.2]\n",
      " [0.3 0.7]\n",
      " [0.3 0.7]]\n",
      "ROC:0.0274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = model.predict_proba(X_test)\n",
    "print(Y_test_pred)\n",
    "\n",
    "roc = roc_auc_score(y_test, Y_test_pred[:,0])\n",
    "print(\"ROC:\" + str(round(roc,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_predClass = model.predict_proba(test_dataX)\n",
    "print(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_test_predClass[:,0]).to_csv('submission.csv', header=[\"Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataX = np.load('train_data.npy')\n",
    "test_dataX = np.load('test_data.npy')\n",
    "train_datay = pd.read_csv('train_labels.csv')\n",
    "\n",
    "train_datay = train_datay.drop('Id',1)\n",
    "\n",
    "#splitting data for validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dataX,train_datay, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBoostClassifier(verbose=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = model.predict_proba(X_test)\n",
    "print(Y_test_pred)\n",
    "\n",
    "roc = roc_auc_score(y_test, Y_test_pred)\n",
    "print(\"ROC:\" + str(round(roc,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_predClass = model.predict_proba(test_dataX)\n",
    "print(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_test_predClass[:,0]).to_csv('submission19.csv', header=[\"Label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
